{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1414,
   "id": "4e8e5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "id": "db2f52b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aungmyinmoe/Documents/NUS-ISS GDipSA/SA60 Materials/SA4110 Machine Learning Applications/CA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "id": "03e3698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5)\n",
    "        # self.conv5 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=9*9*512, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        \n",
    "        # # Define a projection for x1 to match the shape of x before addition\n",
    "        # self.shortcut = nn.Sequential(\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=1),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),  # mimic the same pooling\n",
    "        # )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolution layer 1\n",
    "        x1 = self.conv1(x)\n",
    "        x = self.relu(x1)\n",
    "        \n",
    "        # convolution layer 2\n",
    "        x = self.conv2(self.dropout(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # convolution layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Match dimensions of x1\n",
    "        # x1_proj = self.shortcut(x1)\n",
    "        \n",
    "        # convolution layer 4\n",
    "        x = self.conv4(self.dropout(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # convolution layer 5\n",
    "        # x = self.conv5(x)\n",
    "        # x = self.relu(x)\n",
    "        \n",
    "        # flattening\n",
    "        x = x.view(-1, 9*9*512)\n",
    "        \n",
    "        # dense layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # dense layer 1\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # dense layer 2\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "id": "0b94bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# instantiate the model, define the loss function and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) # use Adam algo to optimize gradient descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1418,
   "id": "00729fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#print(os.getcwd()) # find out current working directory in notebook\n",
    "\n",
    "# prepare all the image file paths and labels\n",
    "def load_filepaths(target_dir): \n",
    "    paths = []\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")\n",
    "    files = os.listdir(target_dir)\n",
    "    for file in files:\n",
    "        if file.endswith(valid_extensions):\n",
    "            paths.append(f\"{target_dir}/{file}\")    # list all file names in the folder\n",
    "    return paths\n",
    "\n",
    "def prepare_data(target_dir):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(4):\n",
    "        fpaths = load_filepaths(target_dir + str(i))\n",
    "        labels += [i] * len(fpaths)\n",
    "        filepaths += fpaths # += add elements individually. append() will add the entire list\n",
    "\n",
    "    return np.array(filepaths), torch.tensor(labels)\n",
    "\n",
    "dir_train = \"train/\"\n",
    "filepaths, labels = prepare_data(dir_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "id": "7b3b6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# prepare to load images when testing\n",
    "def load_images(filepaths):\n",
    "    # Instantiate class to resize and transform image to tensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        # transforms.RandomResizedCrop(48, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),  # Convert PIL image to tensor (C x H x W)\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # default is 0.5\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ])\n",
    "\n",
    "    tensor = None\n",
    "\n",
    "    # List all files in the directory\n",
    "    for item in filepaths:\n",
    "        image = Image.open(item).convert(\"RGB\")     # force 3 channels\n",
    "        #print(f\"image size = {image.size}\")\n",
    "\n",
    "        # transforms.ToTensor() performs transformations on images\n",
    "        # values of img_tensor are in the range of [0.0, 1.0]\n",
    "        img_tensor = transform(image) # convert into pytorch's tensor to work with\n",
    "        #print(f\"img_tensor.shape = {img_tensor.shape}\")\n",
    "        #input()\n",
    "\n",
    "        if tensor is None:\n",
    "            # size: [1,1,28,28]\n",
    "            tensor = img_tensor.unsqueeze(0) # add a new dimension at specified index. in this case, added dimension is for batch dimension\n",
    "        else:\n",
    "            # concatenate becomes [2,1,28,28], [3,1,28,28], [4,1,28,28] ...\n",
    "            # dim=0 concatenates along the axis=0 (row-wise)\n",
    "            tensor = torch.cat((tensor, img_tensor.unsqueeze(0)), dim=0)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "id": "50481122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to load images when training\n",
    "def load_test_images(filepaths):\n",
    "    # Instantiate class to resize and transform image to tensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),  # Convert PIL image to tensor (C x H x W)\n",
    "    ])\n",
    "\n",
    "    tensor = None\n",
    "\n",
    "    # List all files in the directory\n",
    "    for item in filepaths:\n",
    "        image = Image.open(item).convert(\"RGB\")     # force 3 channels\n",
    "        img_tensor = transform(image) # convert into pytorch's tensor to work with\n",
    "\n",
    "\n",
    "        if tensor is None:\n",
    "            # size: [1,1,28,28]\n",
    "            tensor = img_tensor.unsqueeze(0) # add a new dimension at specified index. in this case, added dimension is for batch dimension\n",
    "        else:\n",
    "            # concatenate becomes [2,1,28,28], [3,1,28,28], [4,1,28,28] ...\n",
    "            # dim=0 concatenates along the axis=0 (row-wise)\n",
    "            tensor = torch.cat((tensor, img_tensor.unsqueeze(0)), dim=0)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1421,
   "id": "ed7c839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (32/262): Loss=1.40839, Accuracy=0.28125\n",
      "Epoch 1 (64/262): Loss=1.43407, Accuracy=0.32812\n",
      "Epoch 1 (96/262): Loss=1.42482, Accuracy=0.26042\n",
      "Epoch 1 (128/262): Loss=1.41432, Accuracy=0.28125\n",
      "Epoch 1 (160/262): Loss=1.40595, Accuracy=0.28750\n",
      "Epoch 1 (192/262): Loss=1.40484, Accuracy=0.26562\n",
      "Epoch 1 (224/262): Loss=1.40255, Accuracy=0.27679\n",
      "Epoch 1 (256/262): Loss=1.40610, Accuracy=0.26172\n",
      "Epoch 1 (262/262): Loss=1.40578, Accuracy=0.25954\n",
      "Epoch 2 (32/262): Loss=1.39050, Accuracy=0.34375\n",
      "Epoch 2 (64/262): Loss=1.39373, Accuracy=0.31250\n",
      "Epoch 2 (96/262): Loss=1.38938, Accuracy=0.27083\n",
      "Epoch 2 (128/262): Loss=1.38799, Accuracy=0.28906\n",
      "Epoch 2 (160/262): Loss=1.38824, Accuracy=0.28125\n",
      "Epoch 2 (192/262): Loss=1.38958, Accuracy=0.28125\n",
      "Epoch 2 (224/262): Loss=1.39491, Accuracy=0.25446\n",
      "Epoch 2 (256/262): Loss=1.39415, Accuracy=0.25781\n",
      "Epoch 2 (262/262): Loss=1.39446, Accuracy=0.25954\n",
      "Epoch 3 (32/262): Loss=1.39988, Accuracy=0.21875\n",
      "Epoch 3 (64/262): Loss=1.39969, Accuracy=0.20312\n",
      "Epoch 3 (96/262): Loss=1.39442, Accuracy=0.22917\n",
      "Epoch 3 (128/262): Loss=1.38451, Accuracy=0.28125\n",
      "Epoch 3 (160/262): Loss=1.38665, Accuracy=0.25625\n",
      "Epoch 3 (192/262): Loss=1.38714, Accuracy=0.25521\n",
      "Epoch 3 (224/262): Loss=1.38995, Accuracy=0.24554\n",
      "Epoch 3 (256/262): Loss=1.38965, Accuracy=0.23438\n",
      "Epoch 3 (262/262): Loss=1.39006, Accuracy=0.23664\n",
      "Epoch 4 (32/262): Loss=1.38633, Accuracy=0.21875\n",
      "Epoch 4 (64/262): Loss=1.38010, Accuracy=0.25000\n",
      "Epoch 4 (96/262): Loss=1.38313, Accuracy=0.20833\n",
      "Epoch 4 (128/262): Loss=1.38886, Accuracy=0.18750\n",
      "Epoch 4 (160/262): Loss=1.38338, Accuracy=0.22500\n",
      "Epoch 4 (192/262): Loss=1.38658, Accuracy=0.21875\n",
      "Epoch 4 (224/262): Loss=1.38309, Accuracy=0.25000\n",
      "Epoch 4 (256/262): Loss=1.38189, Accuracy=0.25391\n",
      "Epoch 4 (262/262): Loss=1.38166, Accuracy=0.25573\n",
      "Epoch 5 (32/262): Loss=1.38463, Accuracy=0.15625\n",
      "Epoch 5 (64/262): Loss=1.38486, Accuracy=0.14062\n",
      "Epoch 5 (96/262): Loss=1.37842, Accuracy=0.23958\n",
      "Epoch 5 (128/262): Loss=1.37812, Accuracy=0.26562\n",
      "Epoch 5 (160/262): Loss=1.37232, Accuracy=0.25000\n",
      "Epoch 5 (192/262): Loss=1.37387, Accuracy=0.22917\n",
      "Epoch 5 (224/262): Loss=1.38094, Accuracy=0.23214\n",
      "Epoch 5 (256/262): Loss=1.37343, Accuracy=0.25000\n",
      "Epoch 5 (262/262): Loss=1.37288, Accuracy=0.25191\n",
      "Epoch 6 (32/262): Loss=1.38367, Accuracy=0.31250\n",
      "Epoch 6 (64/262): Loss=1.36543, Accuracy=0.40625\n",
      "Epoch 6 (96/262): Loss=1.35472, Accuracy=0.38542\n",
      "Epoch 6 (128/262): Loss=1.36162, Accuracy=0.36719\n",
      "Epoch 6 (160/262): Loss=1.35658, Accuracy=0.36250\n",
      "Epoch 6 (192/262): Loss=1.35305, Accuracy=0.38021\n",
      "Epoch 6 (224/262): Loss=1.34450, Accuracy=0.39732\n",
      "Epoch 6 (256/262): Loss=1.33283, Accuracy=0.42188\n",
      "Epoch 6 (262/262): Loss=1.33017, Accuracy=0.42748\n",
      "Epoch 7 (32/262): Loss=1.26389, Accuracy=0.46875\n",
      "Epoch 7 (64/262): Loss=1.29457, Accuracy=0.43750\n",
      "Epoch 7 (96/262): Loss=1.27356, Accuracy=0.43750\n",
      "Epoch 7 (128/262): Loss=1.26118, Accuracy=0.44531\n",
      "Epoch 7 (160/262): Loss=1.23763, Accuracy=0.45625\n",
      "Epoch 7 (192/262): Loss=1.25374, Accuracy=0.44792\n",
      "Epoch 7 (224/262): Loss=1.23410, Accuracy=0.45089\n",
      "Epoch 7 (256/262): Loss=1.20871, Accuracy=0.46094\n",
      "Epoch 7 (262/262): Loss=1.21125, Accuracy=0.46183\n",
      "Epoch 8 (32/262): Loss=1.07416, Accuracy=0.50000\n",
      "Epoch 8 (64/262): Loss=1.04866, Accuracy=0.54688\n",
      "Epoch 8 (96/262): Loss=1.12244, Accuracy=0.47917\n",
      "Epoch 8 (128/262): Loss=1.07154, Accuracy=0.51562\n",
      "Epoch 8 (160/262): Loss=1.05708, Accuracy=0.51875\n",
      "Epoch 8 (192/262): Loss=1.05472, Accuracy=0.52083\n",
      "Epoch 8 (224/262): Loss=1.09835, Accuracy=0.49107\n",
      "Epoch 8 (256/262): Loss=1.09063, Accuracy=0.50000\n",
      "Epoch 8 (262/262): Loss=1.08893, Accuracy=0.50000\n",
      "Epoch 9 (32/262): Loss=1.08733, Accuracy=0.43750\n",
      "Epoch 9 (64/262): Loss=1.10579, Accuracy=0.48438\n",
      "Epoch 9 (96/262): Loss=1.06631, Accuracy=0.50000\n",
      "Epoch 9 (128/262): Loss=1.03668, Accuracy=0.54688\n",
      "Epoch 9 (160/262): Loss=1.01962, Accuracy=0.55625\n",
      "Epoch 9 (192/262): Loss=1.01064, Accuracy=0.55729\n",
      "Epoch 9 (224/262): Loss=0.99160, Accuracy=0.57589\n",
      "Epoch 9 (256/262): Loss=0.98099, Accuracy=0.57812\n",
      "Epoch 9 (262/262): Loss=0.97573, Accuracy=0.58015\n",
      "Epoch 10 (32/262): Loss=1.05001, Accuracy=0.59375\n",
      "Epoch 10 (64/262): Loss=0.95591, Accuracy=0.62500\n",
      "Epoch 10 (96/262): Loss=0.93660, Accuracy=0.59375\n",
      "Epoch 10 (128/262): Loss=0.91508, Accuracy=0.62500\n",
      "Epoch 10 (160/262): Loss=0.95429, Accuracy=0.60000\n",
      "Epoch 10 (192/262): Loss=0.94522, Accuracy=0.60417\n",
      "Epoch 10 (224/262): Loss=0.91520, Accuracy=0.62500\n",
      "Epoch 10 (256/262): Loss=0.92013, Accuracy=0.61328\n",
      "Epoch 10 (262/262): Loss=0.92166, Accuracy=0.61069\n",
      "Epoch 11 (32/262): Loss=0.70301, Accuracy=0.75000\n",
      "Epoch 11 (64/262): Loss=0.79614, Accuracy=0.68750\n",
      "Epoch 11 (96/262): Loss=0.80678, Accuracy=0.65625\n",
      "Epoch 11 (128/262): Loss=0.87382, Accuracy=0.62500\n",
      "Epoch 11 (160/262): Loss=0.87206, Accuracy=0.63125\n",
      "Epoch 11 (192/262): Loss=0.82855, Accuracy=0.65625\n",
      "Epoch 11 (224/262): Loss=0.85817, Accuracy=0.63393\n",
      "Epoch 11 (256/262): Loss=0.90129, Accuracy=0.60938\n",
      "Epoch 11 (262/262): Loss=0.90232, Accuracy=0.61069\n",
      "Epoch 12 (32/262): Loss=1.02063, Accuracy=0.53125\n",
      "Epoch 12 (64/262): Loss=0.78561, Accuracy=0.65625\n",
      "Epoch 12 (96/262): Loss=0.84527, Accuracy=0.62500\n",
      "Epoch 12 (128/262): Loss=0.88883, Accuracy=0.60156\n",
      "Epoch 12 (160/262): Loss=0.82828, Accuracy=0.63125\n",
      "Epoch 12 (192/262): Loss=0.81637, Accuracy=0.63542\n",
      "Epoch 12 (224/262): Loss=0.81377, Accuracy=0.64732\n",
      "Epoch 12 (256/262): Loss=0.80732, Accuracy=0.65625\n",
      "Epoch 12 (262/262): Loss=0.81907, Accuracy=0.64885\n",
      "Epoch 13 (32/262): Loss=0.70673, Accuracy=0.78125\n",
      "Epoch 13 (64/262): Loss=0.67475, Accuracy=0.75000\n",
      "Epoch 13 (96/262): Loss=0.66154, Accuracy=0.76042\n",
      "Epoch 13 (128/262): Loss=0.71460, Accuracy=0.75000\n",
      "Epoch 13 (160/262): Loss=0.79270, Accuracy=0.71250\n",
      "Epoch 13 (192/262): Loss=0.82711, Accuracy=0.70312\n",
      "Epoch 13 (224/262): Loss=0.80707, Accuracy=0.70536\n",
      "Epoch 13 (256/262): Loss=0.81643, Accuracy=0.69531\n",
      "Epoch 13 (262/262): Loss=0.81406, Accuracy=0.69847\n",
      "Epoch 14 (32/262): Loss=0.66059, Accuracy=0.62500\n",
      "Epoch 14 (64/262): Loss=0.69633, Accuracy=0.62500\n",
      "Epoch 14 (96/262): Loss=0.70086, Accuracy=0.63542\n",
      "Epoch 14 (128/262): Loss=0.75057, Accuracy=0.62500\n",
      "Epoch 14 (160/262): Loss=0.70859, Accuracy=0.66875\n",
      "Epoch 14 (192/262): Loss=0.70926, Accuracy=0.66667\n",
      "Epoch 14 (224/262): Loss=0.72950, Accuracy=0.68750\n",
      "Epoch 14 (256/262): Loss=0.73893, Accuracy=0.68750\n",
      "Epoch 14 (262/262): Loss=0.73895, Accuracy=0.68321\n",
      "Epoch 15 (32/262): Loss=0.68144, Accuracy=0.75000\n",
      "Epoch 15 (64/262): Loss=0.74246, Accuracy=0.71875\n",
      "Epoch 15 (96/262): Loss=0.69006, Accuracy=0.75000\n",
      "Epoch 15 (128/262): Loss=0.67235, Accuracy=0.75781\n",
      "Epoch 15 (160/262): Loss=0.65118, Accuracy=0.76250\n",
      "Epoch 15 (192/262): Loss=0.67281, Accuracy=0.74479\n",
      "Epoch 15 (224/262): Loss=0.65068, Accuracy=0.74554\n",
      "Epoch 15 (256/262): Loss=0.65905, Accuracy=0.74609\n",
      "Epoch 15 (262/262): Loss=0.66987, Accuracy=0.74427\n",
      "Epoch 16 (32/262): Loss=0.48952, Accuracy=0.78125\n",
      "Epoch 16 (64/262): Loss=0.78286, Accuracy=0.67188\n",
      "Epoch 16 (96/262): Loss=0.74976, Accuracy=0.66667\n",
      "Epoch 16 (128/262): Loss=0.76559, Accuracy=0.64844\n",
      "Epoch 16 (160/262): Loss=0.76506, Accuracy=0.64375\n",
      "Epoch 16 (192/262): Loss=0.72670, Accuracy=0.66667\n",
      "Epoch 16 (224/262): Loss=0.71773, Accuracy=0.67411\n",
      "Epoch 16 (256/262): Loss=0.70118, Accuracy=0.69531\n",
      "Epoch 16 (262/262): Loss=0.72249, Accuracy=0.69466\n",
      "Epoch 17 (32/262): Loss=0.55150, Accuracy=0.78125\n",
      "Epoch 17 (64/262): Loss=0.63253, Accuracy=0.75000\n",
      "Epoch 17 (96/262): Loss=0.66758, Accuracy=0.72917\n",
      "Epoch 17 (128/262): Loss=0.62194, Accuracy=0.75000\n",
      "Epoch 17 (160/262): Loss=0.64738, Accuracy=0.74375\n",
      "Epoch 17 (192/262): Loss=0.62691, Accuracy=0.75000\n",
      "Epoch 17 (224/262): Loss=0.63699, Accuracy=0.74107\n",
      "Epoch 17 (256/262): Loss=0.61055, Accuracy=0.76172\n",
      "Epoch 17 (262/262): Loss=0.60798, Accuracy=0.76336\n",
      "Epoch 18 (32/262): Loss=0.54040, Accuracy=0.75000\n",
      "Epoch 18 (64/262): Loss=0.55998, Accuracy=0.75000\n",
      "Epoch 18 (96/262): Loss=0.58002, Accuracy=0.75000\n",
      "Epoch 18 (128/262): Loss=0.55202, Accuracy=0.75000\n",
      "Epoch 18 (160/262): Loss=0.61359, Accuracy=0.72500\n",
      "Epoch 18 (192/262): Loss=0.59872, Accuracy=0.72917\n",
      "Epoch 18 (224/262): Loss=0.59344, Accuracy=0.72321\n",
      "Epoch 18 (256/262): Loss=0.57056, Accuracy=0.74219\n",
      "Epoch 18 (262/262): Loss=0.56751, Accuracy=0.74427\n",
      "Epoch 19 (32/262): Loss=0.54906, Accuracy=0.71875\n",
      "Epoch 19 (64/262): Loss=0.45245, Accuracy=0.79688\n",
      "Epoch 19 (96/262): Loss=0.47158, Accuracy=0.79167\n",
      "Epoch 19 (128/262): Loss=0.44769, Accuracy=0.81250\n",
      "Epoch 19 (160/262): Loss=0.52405, Accuracy=0.77500\n",
      "Epoch 19 (192/262): Loss=0.49225, Accuracy=0.79167\n",
      "Epoch 19 (224/262): Loss=0.49960, Accuracy=0.79018\n",
      "Epoch 19 (256/262): Loss=0.49724, Accuracy=0.78906\n",
      "Epoch 19 (262/262): Loss=0.49675, Accuracy=0.79008\n",
      "Epoch 20 (32/262): Loss=0.45318, Accuracy=0.84375\n",
      "Epoch 20 (64/262): Loss=0.51379, Accuracy=0.81250\n",
      "Epoch 20 (96/262): Loss=0.47542, Accuracy=0.83333\n",
      "Epoch 20 (128/262): Loss=0.46901, Accuracy=0.82812\n",
      "Epoch 20 (160/262): Loss=0.44180, Accuracy=0.83125\n",
      "Epoch 20 (192/262): Loss=0.42073, Accuracy=0.83333\n",
      "Epoch 20 (224/262): Loss=0.45349, Accuracy=0.81696\n",
      "Epoch 20 (256/262): Loss=0.43816, Accuracy=0.82031\n",
      "Epoch 20 (262/262): Loss=0.42961, Accuracy=0.82443\n",
      "Epoch 21 (32/262): Loss=0.37480, Accuracy=0.84375\n",
      "Epoch 21 (64/262): Loss=0.40188, Accuracy=0.81250\n",
      "Epoch 21 (96/262): Loss=0.40296, Accuracy=0.81250\n",
      "Epoch 21 (128/262): Loss=0.35796, Accuracy=0.84375\n",
      "Epoch 21 (160/262): Loss=0.40468, Accuracy=0.82500\n",
      "Epoch 21 (192/262): Loss=0.38973, Accuracy=0.83333\n",
      "Epoch 21 (224/262): Loss=0.40011, Accuracy=0.83036\n",
      "Epoch 21 (256/262): Loss=0.39878, Accuracy=0.82812\n",
      "Epoch 21 (262/262): Loss=0.40733, Accuracy=0.82443\n",
      "Epoch 22 (32/262): Loss=0.26844, Accuracy=0.90625\n",
      "Epoch 22 (64/262): Loss=0.30790, Accuracy=0.89062\n",
      "Epoch 22 (96/262): Loss=0.37289, Accuracy=0.84375\n",
      "Epoch 22 (128/262): Loss=0.50452, Accuracy=0.78906\n",
      "Epoch 22 (160/262): Loss=0.45051, Accuracy=0.81875\n",
      "Epoch 22 (192/262): Loss=0.43124, Accuracy=0.82812\n",
      "Epoch 22 (224/262): Loss=0.42158, Accuracy=0.83482\n",
      "Epoch 22 (256/262): Loss=0.40833, Accuracy=0.83984\n",
      "Epoch 22 (262/262): Loss=0.41156, Accuracy=0.83969\n",
      "Epoch 23 (32/262): Loss=0.44160, Accuracy=0.87500\n",
      "Epoch 23 (64/262): Loss=0.43060, Accuracy=0.85938\n",
      "Epoch 23 (96/262): Loss=0.44172, Accuracy=0.83333\n",
      "Epoch 23 (128/262): Loss=0.47103, Accuracy=0.81250\n",
      "Epoch 23 (160/262): Loss=0.45496, Accuracy=0.82500\n",
      "Epoch 23 (192/262): Loss=0.43190, Accuracy=0.83333\n",
      "Epoch 23 (224/262): Loss=0.42876, Accuracy=0.83929\n",
      "Epoch 23 (256/262): Loss=0.40730, Accuracy=0.85156\n",
      "Epoch 23 (262/262): Loss=0.42251, Accuracy=0.84733\n",
      "Epoch 24 (32/262): Loss=0.41042, Accuracy=0.93750\n",
      "Epoch 24 (64/262): Loss=0.39875, Accuracy=0.87500\n",
      "Epoch 24 (96/262): Loss=0.41112, Accuracy=0.85417\n",
      "Epoch 24 (128/262): Loss=0.46886, Accuracy=0.84375\n",
      "Epoch 24 (160/262): Loss=0.53431, Accuracy=0.80000\n",
      "Epoch 24 (192/262): Loss=0.53449, Accuracy=0.80208\n",
      "Epoch 24 (224/262): Loss=0.49836, Accuracy=0.81696\n",
      "Epoch 24 (256/262): Loss=0.48380, Accuracy=0.83203\n",
      "Epoch 24 (262/262): Loss=0.48397, Accuracy=0.83206\n",
      "Epoch 25 (32/262): Loss=0.33963, Accuracy=0.87500\n",
      "Epoch 25 (64/262): Loss=0.42752, Accuracy=0.82812\n",
      "Epoch 25 (96/262): Loss=0.53965, Accuracy=0.78125\n",
      "Epoch 25 (128/262): Loss=0.52307, Accuracy=0.77344\n",
      "Epoch 25 (160/262): Loss=0.49964, Accuracy=0.80000\n",
      "Epoch 25 (192/262): Loss=0.50583, Accuracy=0.79167\n",
      "Epoch 25 (224/262): Loss=0.49828, Accuracy=0.79018\n",
      "Epoch 25 (256/262): Loss=0.48145, Accuracy=0.80078\n",
      "Epoch 25 (262/262): Loss=0.48819, Accuracy=0.80153\n",
      "Epoch 26 (32/262): Loss=0.42082, Accuracy=0.84375\n",
      "Epoch 26 (64/262): Loss=0.39532, Accuracy=0.84375\n",
      "Epoch 26 (96/262): Loss=0.41525, Accuracy=0.82292\n",
      "Epoch 26 (128/262): Loss=0.37375, Accuracy=0.84375\n",
      "Epoch 26 (160/262): Loss=0.37776, Accuracy=0.85000\n",
      "Epoch 26 (192/262): Loss=0.37382, Accuracy=0.85417\n",
      "Epoch 26 (224/262): Loss=0.36441, Accuracy=0.85714\n",
      "Epoch 26 (256/262): Loss=0.34808, Accuracy=0.86719\n",
      "Epoch 26 (262/262): Loss=0.34473, Accuracy=0.87023\n",
      "Epoch 27 (32/262): Loss=0.30351, Accuracy=0.90625\n",
      "Epoch 27 (64/262): Loss=0.31383, Accuracy=0.89062\n",
      "Epoch 27 (96/262): Loss=0.29541, Accuracy=0.88542\n",
      "Epoch 27 (128/262): Loss=0.29939, Accuracy=0.89062\n",
      "Epoch 27 (160/262): Loss=0.30699, Accuracy=0.88125\n",
      "Epoch 27 (192/262): Loss=0.33925, Accuracy=0.86458\n",
      "Epoch 27 (224/262): Loss=0.35322, Accuracy=0.85714\n",
      "Epoch 27 (256/262): Loss=0.34918, Accuracy=0.85938\n",
      "Epoch 27 (262/262): Loss=0.35189, Accuracy=0.85878\n",
      "Epoch 28 (32/262): Loss=0.31665, Accuracy=0.84375\n",
      "Epoch 28 (64/262): Loss=0.28386, Accuracy=0.89062\n",
      "Epoch 28 (96/262): Loss=0.28752, Accuracy=0.87500\n",
      "Epoch 28 (128/262): Loss=0.30433, Accuracy=0.87500\n",
      "Epoch 28 (160/262): Loss=0.29097, Accuracy=0.89375\n",
      "Epoch 28 (192/262): Loss=0.30580, Accuracy=0.89062\n",
      "Epoch 28 (224/262): Loss=0.28828, Accuracy=0.89732\n",
      "Epoch 28 (256/262): Loss=0.28427, Accuracy=0.89844\n",
      "Epoch 28 (262/262): Loss=0.28179, Accuracy=0.90076\n",
      "Epoch 29 (32/262): Loss=0.21243, Accuracy=0.90625\n",
      "Epoch 29 (64/262): Loss=0.23235, Accuracy=0.92188\n",
      "Epoch 29 (96/262): Loss=0.20809, Accuracy=0.93750\n",
      "Epoch 29 (128/262): Loss=0.21267, Accuracy=0.92969\n",
      "Epoch 29 (160/262): Loss=0.20207, Accuracy=0.92500\n",
      "Epoch 29 (192/262): Loss=0.20565, Accuracy=0.92188\n",
      "Epoch 29 (224/262): Loss=0.22972, Accuracy=0.91518\n",
      "Epoch 29 (256/262): Loss=0.23051, Accuracy=0.91797\n",
      "Epoch 29 (262/262): Loss=0.22559, Accuracy=0.91985\n",
      "Epoch 30 (32/262): Loss=0.19645, Accuracy=0.93750\n",
      "Epoch 30 (64/262): Loss=0.16181, Accuracy=0.93750\n",
      "Epoch 30 (96/262): Loss=0.16202, Accuracy=0.94792\n",
      "Epoch 30 (128/262): Loss=0.15610, Accuracy=0.94531\n",
      "Epoch 30 (160/262): Loss=0.16428, Accuracy=0.94375\n",
      "Epoch 30 (192/262): Loss=0.20967, Accuracy=0.92708\n",
      "Epoch 30 (224/262): Loss=0.19820, Accuracy=0.93304\n",
      "Epoch 30 (256/262): Loss=0.22249, Accuracy=0.92969\n",
      "Epoch 30 (262/262): Loss=0.22101, Accuracy=0.93130\n",
      "Epoch 31 (32/262): Loss=0.20838, Accuracy=0.90625\n",
      "Epoch 31 (64/262): Loss=0.28684, Accuracy=0.87500\n",
      "Epoch 31 (96/262): Loss=0.24486, Accuracy=0.89583\n",
      "Epoch 31 (128/262): Loss=0.22064, Accuracy=0.91406\n",
      "Epoch 31 (160/262): Loss=0.19966, Accuracy=0.91875\n",
      "Epoch 31 (192/262): Loss=0.18716, Accuracy=0.92708\n",
      "Epoch 31 (224/262): Loss=0.23306, Accuracy=0.91071\n",
      "Epoch 31 (256/262): Loss=0.22914, Accuracy=0.91016\n",
      "Epoch 31 (262/262): Loss=0.22782, Accuracy=0.90840\n",
      "Epoch 32 (32/262): Loss=0.11378, Accuracy=0.96875\n",
      "Epoch 32 (64/262): Loss=0.13557, Accuracy=0.92188\n",
      "Epoch 32 (96/262): Loss=0.20857, Accuracy=0.90625\n",
      "Epoch 32 (128/262): Loss=0.21528, Accuracy=0.90625\n",
      "Epoch 32 (160/262): Loss=0.20459, Accuracy=0.91875\n",
      "Epoch 32 (192/262): Loss=0.20976, Accuracy=0.92188\n",
      "Epoch 32 (224/262): Loss=0.24109, Accuracy=0.90179\n",
      "Epoch 32 (256/262): Loss=0.24264, Accuracy=0.89844\n",
      "Epoch 32 (262/262): Loss=0.23881, Accuracy=0.90076\n",
      "Epoch 33 (32/262): Loss=0.10170, Accuracy=0.96875\n",
      "Epoch 33 (64/262): Loss=0.21385, Accuracy=0.89062\n",
      "Epoch 33 (96/262): Loss=0.25568, Accuracy=0.88542\n",
      "Epoch 33 (128/262): Loss=0.24836, Accuracy=0.89844\n",
      "Epoch 33 (160/262): Loss=0.22354, Accuracy=0.90625\n",
      "Epoch 33 (192/262): Loss=0.21422, Accuracy=0.91146\n",
      "Epoch 33 (224/262): Loss=0.21107, Accuracy=0.91071\n",
      "Epoch 33 (256/262): Loss=0.23812, Accuracy=0.90625\n",
      "Epoch 33 (262/262): Loss=0.24126, Accuracy=0.90458\n",
      "Epoch 34 (32/262): Loss=0.55303, Accuracy=0.84375\n",
      "Epoch 34 (64/262): Loss=0.48635, Accuracy=0.84375\n",
      "Epoch 34 (96/262): Loss=0.47861, Accuracy=0.85417\n",
      "Epoch 34 (128/262): Loss=0.48967, Accuracy=0.84375\n",
      "Epoch 34 (160/262): Loss=0.43547, Accuracy=0.86250\n",
      "Epoch 34 (192/262): Loss=0.38558, Accuracy=0.87500\n",
      "Epoch 34 (224/262): Loss=0.37896, Accuracy=0.87500\n",
      "Epoch 34 (256/262): Loss=0.35501, Accuracy=0.87891\n",
      "Epoch 34 (262/262): Loss=0.35277, Accuracy=0.87786\n",
      "Epoch 35 (32/262): Loss=0.34556, Accuracy=0.84375\n",
      "Epoch 35 (64/262): Loss=0.32448, Accuracy=0.84375\n",
      "Epoch 35 (96/262): Loss=0.28161, Accuracy=0.86458\n",
      "Epoch 35 (128/262): Loss=0.25780, Accuracy=0.88281\n",
      "Epoch 35 (160/262): Loss=0.23707, Accuracy=0.90000\n",
      "Epoch 35 (192/262): Loss=0.23059, Accuracy=0.90104\n",
      "Epoch 35 (224/262): Loss=0.21385, Accuracy=0.91518\n",
      "Epoch 35 (256/262): Loss=0.21597, Accuracy=0.91797\n",
      "Epoch 35 (262/262): Loss=0.21333, Accuracy=0.91985\n",
      "Epoch 36 (32/262): Loss=0.22535, Accuracy=0.90625\n",
      "Epoch 36 (64/262): Loss=0.16484, Accuracy=0.93750\n",
      "Epoch 36 (96/262): Loss=0.19686, Accuracy=0.90625\n",
      "Epoch 36 (128/262): Loss=0.19391, Accuracy=0.90625\n",
      "Epoch 36 (160/262): Loss=0.17481, Accuracy=0.91875\n",
      "Epoch 36 (192/262): Loss=0.20982, Accuracy=0.90625\n",
      "Epoch 36 (224/262): Loss=0.22524, Accuracy=0.89732\n",
      "Epoch 36 (256/262): Loss=0.22296, Accuracy=0.90234\n",
      "Epoch 36 (262/262): Loss=0.21817, Accuracy=0.90458\n",
      "Epoch 37 (32/262): Loss=0.19428, Accuracy=0.93750\n",
      "Epoch 37 (64/262): Loss=0.22673, Accuracy=0.93750\n",
      "Epoch 37 (96/262): Loss=0.20079, Accuracy=0.93750\n",
      "Epoch 37 (128/262): Loss=0.20300, Accuracy=0.92969\n",
      "Epoch 37 (160/262): Loss=0.19035, Accuracy=0.93750\n",
      "Epoch 37 (192/262): Loss=0.21876, Accuracy=0.93229\n",
      "Epoch 37 (224/262): Loss=0.23470, Accuracy=0.92411\n",
      "Epoch 37 (256/262): Loss=0.22777, Accuracy=0.92969\n",
      "Epoch 37 (262/262): Loss=0.22738, Accuracy=0.93130\n",
      "Epoch 38 (32/262): Loss=0.18762, Accuracy=0.90625\n",
      "Epoch 38 (64/262): Loss=0.18686, Accuracy=0.90625\n",
      "Epoch 38 (96/262): Loss=0.18263, Accuracy=0.91667\n",
      "Epoch 38 (128/262): Loss=0.16813, Accuracy=0.92188\n",
      "Epoch 38 (160/262): Loss=0.18075, Accuracy=0.92500\n",
      "Epoch 38 (192/262): Loss=0.21221, Accuracy=0.92188\n",
      "Epoch 38 (224/262): Loss=0.19489, Accuracy=0.92857\n",
      "Epoch 38 (256/262): Loss=0.18801, Accuracy=0.93359\n",
      "Epoch 38 (262/262): Loss=0.18622, Accuracy=0.93511\n",
      "Epoch 39 (32/262): Loss=0.14472, Accuracy=0.93750\n",
      "Epoch 39 (64/262): Loss=0.09880, Accuracy=0.96875\n",
      "Epoch 39 (96/262): Loss=0.14571, Accuracy=0.93750\n",
      "Epoch 39 (128/262): Loss=0.15147, Accuracy=0.92969\n",
      "Epoch 39 (160/262): Loss=0.14647, Accuracy=0.93125\n",
      "Epoch 39 (192/262): Loss=0.16924, Accuracy=0.92708\n",
      "Epoch 39 (224/262): Loss=0.19739, Accuracy=0.91518\n",
      "Epoch 39 (256/262): Loss=0.21430, Accuracy=0.91406\n",
      "Epoch 39 (262/262): Loss=0.20947, Accuracy=0.91603\n",
      "Epoch 40 (32/262): Loss=0.10111, Accuracy=0.96875\n",
      "Epoch 40 (64/262): Loss=0.11823, Accuracy=0.96875\n",
      "Epoch 40 (96/262): Loss=0.10656, Accuracy=0.97917\n",
      "Epoch 40 (128/262): Loss=0.10794, Accuracy=0.97656\n",
      "Epoch 40 (160/262): Loss=0.12520, Accuracy=0.96875\n",
      "Epoch 40 (192/262): Loss=0.12545, Accuracy=0.96875\n",
      "Epoch 40 (224/262): Loss=0.12155, Accuracy=0.96875\n",
      "Epoch 40 (256/262): Loss=0.11856, Accuracy=0.96875\n",
      "Epoch 40 (262/262): Loss=0.11793, Accuracy=0.96947\n",
      "Epoch 41 (32/262): Loss=0.28424, Accuracy=0.90625\n",
      "Epoch 41 (64/262): Loss=0.19727, Accuracy=0.93750\n",
      "Epoch 41 (96/262): Loss=0.22010, Accuracy=0.92708\n",
      "Epoch 41 (128/262): Loss=0.18112, Accuracy=0.94531\n",
      "Epoch 41 (160/262): Loss=0.16557, Accuracy=0.95000\n",
      "Epoch 41 (192/262): Loss=0.16662, Accuracy=0.94271\n",
      "Epoch 41 (224/262): Loss=0.15402, Accuracy=0.95089\n",
      "Epoch 41 (256/262): Loss=0.15546, Accuracy=0.94531\n",
      "Epoch 41 (262/262): Loss=0.15708, Accuracy=0.94275\n",
      "Epoch 42 (32/262): Loss=0.18280, Accuracy=0.90625\n",
      "Epoch 42 (64/262): Loss=0.18836, Accuracy=0.92188\n",
      "Epoch 42 (96/262): Loss=0.19087, Accuracy=0.92708\n",
      "Epoch 42 (128/262): Loss=0.16276, Accuracy=0.94531\n",
      "Epoch 42 (160/262): Loss=0.15576, Accuracy=0.95000\n",
      "Epoch 42 (192/262): Loss=0.15915, Accuracy=0.94271\n",
      "Epoch 42 (224/262): Loss=0.15964, Accuracy=0.94643\n",
      "Epoch 42 (256/262): Loss=0.15298, Accuracy=0.94922\n",
      "Epoch 42 (262/262): Loss=0.15311, Accuracy=0.95038\n",
      "Epoch 43 (32/262): Loss=0.10365, Accuracy=0.93750\n",
      "Epoch 43 (64/262): Loss=0.11540, Accuracy=0.95312\n",
      "Epoch 43 (96/262): Loss=0.24545, Accuracy=0.91667\n",
      "Epoch 43 (128/262): Loss=0.23464, Accuracy=0.92188\n",
      "Epoch 43 (160/262): Loss=0.20946, Accuracy=0.93125\n",
      "Epoch 43 (192/262): Loss=0.18826, Accuracy=0.93750\n",
      "Epoch 43 (224/262): Loss=0.18334, Accuracy=0.93304\n",
      "Epoch 43 (256/262): Loss=0.17368, Accuracy=0.93359\n",
      "Epoch 43 (262/262): Loss=0.17014, Accuracy=0.93511\n",
      "Epoch 44 (32/262): Loss=0.42953, Accuracy=0.84375\n",
      "Epoch 44 (64/262): Loss=0.26684, Accuracy=0.90625\n",
      "Epoch 44 (96/262): Loss=0.24331, Accuracy=0.91667\n",
      "Epoch 44 (128/262): Loss=0.20815, Accuracy=0.92969\n",
      "Epoch 44 (160/262): Loss=0.22740, Accuracy=0.91250\n",
      "Epoch 44 (192/262): Loss=0.19471, Accuracy=0.92708\n",
      "Epoch 44 (224/262): Loss=0.16973, Accuracy=0.93750\n",
      "Epoch 44 (256/262): Loss=0.16352, Accuracy=0.93359\n",
      "Epoch 44 (262/262): Loss=0.16365, Accuracy=0.93511\n",
      "Epoch 45 (32/262): Loss=0.09120, Accuracy=0.96875\n",
      "Epoch 45 (64/262): Loss=0.10346, Accuracy=0.95312\n",
      "Epoch 45 (96/262): Loss=0.12807, Accuracy=0.94792\n",
      "Epoch 45 (128/262): Loss=0.19118, Accuracy=0.92969\n",
      "Epoch 45 (160/262): Loss=0.18949, Accuracy=0.91250\n",
      "Epoch 45 (192/262): Loss=0.18978, Accuracy=0.91667\n",
      "Epoch 45 (224/262): Loss=0.19782, Accuracy=0.91071\n",
      "Epoch 45 (256/262): Loss=0.19155, Accuracy=0.91797\n",
      "Epoch 45 (262/262): Loss=0.18895, Accuracy=0.91985\n",
      "Epoch 46 (32/262): Loss=0.13135, Accuracy=0.93750\n",
      "Epoch 46 (64/262): Loss=0.17590, Accuracy=0.93750\n",
      "Epoch 46 (96/262): Loss=0.19615, Accuracy=0.92708\n",
      "Epoch 46 (128/262): Loss=0.16822, Accuracy=0.93750\n",
      "Epoch 46 (160/262): Loss=0.17775, Accuracy=0.93125\n",
      "Epoch 46 (192/262): Loss=0.17816, Accuracy=0.92708\n",
      "Epoch 46 (224/262): Loss=0.17182, Accuracy=0.92857\n",
      "Epoch 46 (256/262): Loss=0.17309, Accuracy=0.93359\n",
      "Epoch 46 (262/262): Loss=0.16914, Accuracy=0.93511\n",
      "Epoch 47 (32/262): Loss=0.16953, Accuracy=0.90625\n",
      "Epoch 47 (64/262): Loss=0.16575, Accuracy=0.93750\n",
      "Epoch 47 (96/262): Loss=0.14953, Accuracy=0.93750\n",
      "Epoch 47 (128/262): Loss=0.16498, Accuracy=0.92188\n",
      "Epoch 47 (160/262): Loss=0.14392, Accuracy=0.93750\n",
      "Epoch 47 (192/262): Loss=0.12660, Accuracy=0.94792\n",
      "Epoch 47 (224/262): Loss=0.14298, Accuracy=0.94196\n",
      "Epoch 47 (256/262): Loss=0.15761, Accuracy=0.94141\n",
      "Epoch 47 (262/262): Loss=0.15699, Accuracy=0.93893\n",
      "Epoch 48 (32/262): Loss=0.06539, Accuracy=1.00000\n",
      "Epoch 48 (64/262): Loss=0.06303, Accuracy=0.98438\n",
      "Epoch 48 (96/262): Loss=0.10225, Accuracy=0.96875\n",
      "Epoch 48 (128/262): Loss=0.12750, Accuracy=0.96094\n",
      "Epoch 48 (160/262): Loss=0.14086, Accuracy=0.96250\n",
      "Epoch 48 (192/262): Loss=0.14706, Accuracy=0.95312\n",
      "Epoch 48 (224/262): Loss=0.13916, Accuracy=0.95982\n",
      "Epoch 48 (256/262): Loss=0.13319, Accuracy=0.96094\n",
      "Epoch 48 (262/262): Loss=0.13024, Accuracy=0.96183\n",
      "Epoch 49 (32/262): Loss=0.17374, Accuracy=0.93750\n",
      "Epoch 49 (64/262): Loss=0.12205, Accuracy=0.93750\n",
      "Epoch 49 (96/262): Loss=0.11373, Accuracy=0.94792\n",
      "Epoch 49 (128/262): Loss=0.11509, Accuracy=0.94531\n",
      "Epoch 49 (160/262): Loss=0.10688, Accuracy=0.95000\n",
      "Epoch 49 (192/262): Loss=0.10029, Accuracy=0.94792\n",
      "Epoch 49 (224/262): Loss=0.08880, Accuracy=0.95536\n",
      "Epoch 49 (256/262): Loss=0.09247, Accuracy=0.95312\n",
      "Epoch 49 (262/262): Loss=0.09889, Accuracy=0.95038\n",
      "Epoch 50 (32/262): Loss=0.03540, Accuracy=1.00000\n",
      "Epoch 50 (64/262): Loss=0.18101, Accuracy=0.90625\n",
      "Epoch 50 (96/262): Loss=0.19108, Accuracy=0.91667\n",
      "Epoch 50 (128/262): Loss=0.15259, Accuracy=0.92969\n",
      "Epoch 50 (160/262): Loss=0.14836, Accuracy=0.93125\n",
      "Epoch 50 (192/262): Loss=0.16627, Accuracy=0.92708\n",
      "Epoch 50 (224/262): Loss=0.15662, Accuracy=0.93304\n",
      "Epoch 50 (256/262): Loss=0.15144, Accuracy=0.93750\n",
      "Epoch 50 (262/262): Loss=0.14846, Accuracy=0.93893\n",
      "Epoch 51 (32/262): Loss=0.10793, Accuracy=0.96875\n",
      "Epoch 51 (64/262): Loss=0.13538, Accuracy=0.95312\n",
      "Epoch 51 (96/262): Loss=0.11900, Accuracy=0.95833\n",
      "Epoch 51 (128/262): Loss=0.14846, Accuracy=0.96094\n",
      "Epoch 51 (160/262): Loss=0.14050, Accuracy=0.95625\n",
      "Epoch 51 (192/262): Loss=0.13476, Accuracy=0.95312\n",
      "Epoch 51 (224/262): Loss=0.13234, Accuracy=0.95536\n",
      "Epoch 51 (256/262): Loss=0.13596, Accuracy=0.95312\n",
      "Epoch 51 (262/262): Loss=0.13433, Accuracy=0.95420\n",
      "Epoch 52 (32/262): Loss=0.29096, Accuracy=0.93750\n",
      "Epoch 52 (64/262): Loss=0.20224, Accuracy=0.95312\n",
      "Epoch 52 (96/262): Loss=0.14250, Accuracy=0.96875\n",
      "Epoch 52 (128/262): Loss=0.12537, Accuracy=0.97656\n",
      "Epoch 52 (160/262): Loss=0.12593, Accuracy=0.96875\n",
      "Epoch 52 (192/262): Loss=0.11580, Accuracy=0.96875\n",
      "Epoch 52 (224/262): Loss=0.11712, Accuracy=0.96875\n",
      "Epoch 52 (256/262): Loss=0.11125, Accuracy=0.97266\n",
      "Epoch 52 (262/262): Loss=0.11003, Accuracy=0.97328\n",
      "Epoch 53 (32/262): Loss=0.07673, Accuracy=0.96875\n",
      "Epoch 53 (64/262): Loss=0.10615, Accuracy=0.96875\n",
      "Epoch 53 (96/262): Loss=0.10012, Accuracy=0.95833\n",
      "Epoch 53 (128/262): Loss=0.12526, Accuracy=0.95312\n",
      "Epoch 53 (160/262): Loss=0.11514, Accuracy=0.95000\n",
      "Epoch 53 (192/262): Loss=0.11511, Accuracy=0.94792\n",
      "Epoch 53 (224/262): Loss=0.10907, Accuracy=0.95089\n",
      "Epoch 53 (256/262): Loss=0.10872, Accuracy=0.95312\n",
      "Epoch 53 (262/262): Loss=0.10772, Accuracy=0.95420\n",
      "Epoch 54 (32/262): Loss=0.06706, Accuracy=1.00000\n",
      "Epoch 54 (64/262): Loss=0.04902, Accuracy=1.00000\n",
      "Epoch 54 (96/262): Loss=0.06462, Accuracy=0.98958\n",
      "Epoch 54 (128/262): Loss=0.12122, Accuracy=0.96875\n",
      "Epoch 54 (160/262): Loss=0.12986, Accuracy=0.95625\n",
      "Epoch 54 (192/262): Loss=0.11643, Accuracy=0.96354\n",
      "Epoch 54 (224/262): Loss=0.11199, Accuracy=0.96429\n",
      "Epoch 54 (256/262): Loss=0.10199, Accuracy=0.96875\n",
      "Epoch 54 (262/262): Loss=0.10480, Accuracy=0.96565\n",
      "Epoch 55 (32/262): Loss=0.28120, Accuracy=0.81250\n",
      "Epoch 55 (64/262): Loss=0.19716, Accuracy=0.87500\n",
      "Epoch 55 (96/262): Loss=0.16751, Accuracy=0.90625\n",
      "Epoch 55 (128/262): Loss=0.18712, Accuracy=0.91406\n",
      "Epoch 55 (160/262): Loss=0.18753, Accuracy=0.91875\n",
      "Epoch 55 (192/262): Loss=0.17071, Accuracy=0.92708\n",
      "Epoch 55 (224/262): Loss=0.16063, Accuracy=0.93304\n",
      "Epoch 55 (256/262): Loss=0.15182, Accuracy=0.93750\n",
      "Epoch 55 (262/262): Loss=0.15740, Accuracy=0.93130\n",
      "Epoch 56 (32/262): Loss=0.06607, Accuracy=0.96875\n",
      "Epoch 56 (64/262): Loss=0.04948, Accuracy=0.98438\n",
      "Epoch 56 (96/262): Loss=0.06625, Accuracy=0.97917\n",
      "Epoch 56 (128/262): Loss=0.09841, Accuracy=0.96094\n",
      "Epoch 56 (160/262): Loss=0.17304, Accuracy=0.93750\n",
      "Epoch 56 (192/262): Loss=0.27818, Accuracy=0.91146\n",
      "Epoch 56 (224/262): Loss=0.27495, Accuracy=0.91518\n",
      "Epoch 56 (256/262): Loss=0.31242, Accuracy=0.89844\n",
      "Epoch 56 (262/262): Loss=0.31041, Accuracy=0.89695\n",
      "Epoch 57 (32/262): Loss=0.34228, Accuracy=0.87500\n",
      "Epoch 57 (64/262): Loss=0.23404, Accuracy=0.92188\n",
      "Epoch 57 (96/262): Loss=0.21257, Accuracy=0.92708\n",
      "Epoch 57 (128/262): Loss=0.24512, Accuracy=0.90625\n",
      "Epoch 57 (160/262): Loss=0.24651, Accuracy=0.90625\n",
      "Epoch 57 (192/262): Loss=0.25130, Accuracy=0.89062\n",
      "Epoch 57 (224/262): Loss=0.22158, Accuracy=0.90179\n",
      "Epoch 57 (256/262): Loss=0.21290, Accuracy=0.90234\n",
      "Epoch 57 (262/262): Loss=0.20836, Accuracy=0.90458\n",
      "Epoch 58 (32/262): Loss=0.12090, Accuracy=0.96875\n",
      "Epoch 58 (64/262): Loss=0.08732, Accuracy=0.96875\n",
      "Epoch 58 (96/262): Loss=0.10838, Accuracy=0.95833\n",
      "Epoch 58 (128/262): Loss=0.11130, Accuracy=0.96094\n",
      "Epoch 58 (160/262): Loss=0.11436, Accuracy=0.96250\n",
      "Epoch 58 (192/262): Loss=0.15201, Accuracy=0.95312\n",
      "Epoch 58 (224/262): Loss=0.14219, Accuracy=0.95536\n",
      "Epoch 58 (256/262): Loss=0.15501, Accuracy=0.95312\n",
      "Epoch 58 (262/262): Loss=0.15418, Accuracy=0.95420\n",
      "Epoch 59 (32/262): Loss=0.08335, Accuracy=0.96875\n",
      "Epoch 59 (64/262): Loss=0.07925, Accuracy=0.96875\n",
      "Epoch 59 (96/262): Loss=0.07886, Accuracy=0.96875\n",
      "Epoch 59 (128/262): Loss=0.07767, Accuracy=0.96875\n",
      "Epoch 59 (160/262): Loss=0.12956, Accuracy=0.93750\n",
      "Epoch 59 (192/262): Loss=0.14655, Accuracy=0.93750\n",
      "Epoch 59 (224/262): Loss=0.13222, Accuracy=0.94643\n",
      "Epoch 59 (256/262): Loss=0.14674, Accuracy=0.94141\n",
      "Epoch 59 (262/262): Loss=0.14562, Accuracy=0.94275\n",
      "Epoch 60 (32/262): Loss=0.05757, Accuracy=1.00000\n",
      "Epoch 60 (64/262): Loss=0.07129, Accuracy=0.98438\n",
      "Epoch 60 (96/262): Loss=0.08088, Accuracy=0.97917\n",
      "Epoch 60 (128/262): Loss=0.07600, Accuracy=0.98438\n",
      "Epoch 60 (160/262): Loss=0.06927, Accuracy=0.98750\n",
      "Epoch 60 (192/262): Loss=0.06311, Accuracy=0.98958\n",
      "Epoch 60 (224/262): Loss=0.06871, Accuracy=0.98661\n",
      "Epoch 60 (256/262): Loss=0.08534, Accuracy=0.97656\n",
      "Epoch 60 (262/262): Loss=0.09182, Accuracy=0.97328\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# our hyper-parameters for training\n",
    "n_epochs = 60\n",
    "batch_size = 32\n",
    "\n",
    "# model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # For tracking and printing our training-progress\n",
    "    samples_trained = 0\n",
    "    run_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_samples = len(filepaths) \n",
    "\n",
    "    permutation = torch.randperm(total_samples) # perform permutation for each epoch so that the model does not over-fit or memorize\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        indices = permutation[i : i+batch_size]\n",
    "        batch_inputs = load_images(filepaths[indices])\n",
    "        batch_labels = labels[indices]\n",
    "\n",
    "        # Forward pass: compute predicted outputs\n",
    "        outputs = model(batch_inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        run_loss += loss.item() * len(batch_labels)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()     # compute gradients from back to front\n",
    "        optimizer.step()    # update weights from computed gradients from back to front\n",
    "        \n",
    "        # Get probability-distributions\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(probs, dim=1)  # store index of max probability (_ means ignore the value)\n",
    "\n",
    "        # Calculate some stats\n",
    "        # samples_trained += len(indices)\n",
    "        samples_trained += len(batch_labels)\n",
    "        avg_loss = run_loss / samples_trained\n",
    "\n",
    "        correct_preds += torch.sum(preds == batch_labels) # compare predictions with labels\n",
    "        accuracy = correct_preds / float(samples_trained) # cast to float to get \"accuracy\" in decimal \n",
    "\n",
    "        print(f\"Epoch {epoch+1} \" +\n",
    "            f\"({samples_trained}/{total_samples}): \" +\n",
    "            f\"Loss={avg_loss:.5f}, Accuracy={accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "id": "8fa298d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60/60): Accuracy=0.90000\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "dir_test = \"test/\" \n",
    "filepaths, labels = prepare_data(dir_test)\n",
    "\n",
    "batch_size = 60\n",
    "samples_tested = 0\n",
    "correct_preds = 0\n",
    "total_samples = len(filepaths)\n",
    "\n",
    "model.eval()\n",
    "for i in range(0, total_samples, batch_size):\n",
    "    batch_inputs = load_test_images(filepaths[i : i + batch_size])\n",
    "    batch_labels = labels[i : i + batch_size]\n",
    "\n",
    "    # Forward pass: coyympute predicted outputs\n",
    "    outputs = model(batch_inputs)\n",
    "\n",
    "    # Get probability-distributions\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    _, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    # Determine accuracy\n",
    "    samples_tested += len(batch_labels)\n",
    "    correct_preds += torch.sum(preds == batch_labels)\n",
    "    accuracy = correct_preds / float(samples_tested)\n",
    "\n",
    "    print(f\"({samples_tested}/{total_samples}): Accuracy={accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1423,
   "id": "5d75e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, '90_accuracy_less_complex.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
